# LLM09 â€“ Misinformation

## Summary

Model hallucinations, bias, or credible-sounding false content can cause operational, legal, or reputational harm. Users may act on incorrect information.

## Prevention

- Design for transparency (citations, confidence, uncertainty). Use RAG and grounding where appropriate; warn users about limitations; monitor and correct high-impact outputs.

## Testing

- Evaluate accuracy and citation; test for hallucination and bias; verify user-facing disclaimers.
